{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MHSRBnk4rXM"
      },
      "source": [
        "# Es 1.. 2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "2_eLEehG2sqK"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import json\n",
        "import ijson\n",
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "from decimal import Decimal\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "from itertools import combinations\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ri1ryp964zJQ"
      },
      "outputs": [],
      "source": [
        "citation_graph = nx.DiGraph()\n",
        "collaboration_graph = nx.Graph()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sorted =pd.read_csv('C:/Users/Gabriele/Desktop/universit√†/Magistrale/DATA SCIENCE/ADM/HW5/dataset_sorted_top10000.tsv',sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "ZUPQnunuVFsS",
        "outputId": "c7c20bfc-ce28-4493-b59e-0e8ac25078eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "      <th>authors</th>\n",
              "      <th>n_citation</th>\n",
              "      <th>doc_type</th>\n",
              "      <th>reference_count</th>\n",
              "      <th>references</th>\n",
              "      <th>doi</th>\n",
              "      <th>publisher</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>1964830323</td>\n",
              "      <td>An overview of JML tools and applications</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>[{'name': 'Lilian Burdy', 'org': 'INRIA Sophia...</td>\n",
              "      <td>596.0</td>\n",
              "      <td>Conference</td>\n",
              "      <td>82.0</td>\n",
              "      <td>1486696980;1489778371;1492315860;1498946538;14...</td>\n",
              "      <td>https://doi.org/10.1007/s10009-004-0167-4</td>\n",
              "      <td>Springer-Verlag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>2101699859</td>\n",
              "      <td>Countering code-injection attacks with instruc...</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>[{'name': 'Gaurav S. Kc', 'org': 'Columbia Uni...</td>\n",
              "      <td>596.0</td>\n",
              "      <td>Conference</td>\n",
              "      <td>42.0</td>\n",
              "      <td>186343359;1481758559;1499992849;1508969946;151...</td>\n",
              "      <td>https://doi.org/10.1145/948109.948146</td>\n",
              "      <td>ACM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>2124609748</td>\n",
              "      <td>Gaussian Process Dynamical Models for Human Mo...</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>[{'name': 'J.M. Wang', 'org': 'University of T...</td>\n",
              "      <td>596.0</td>\n",
              "      <td>Conference</td>\n",
              "      <td>61.0</td>\n",
              "      <td>1505866674;1546296670;1598702468;1643263348;19...</td>\n",
              "      <td>https://doi.org/10.1109/TPAMI.2007.1167</td>\n",
              "      <td>IEEE Computer Society</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>2147343704</td>\n",
              "      <td>EVENODD: an efficient scheme for tolerating do...</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>[{'name': 'M. Blaum', 'org': 'IBM Almaden Rese...</td>\n",
              "      <td>596.0</td>\n",
              "      <td>Journal</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1530042190;1531975040;1820898047;1829547464;20...</td>\n",
              "      <td>https://doi.org/10.1109/12.364531</td>\n",
              "      <td>IEEE Computer Society</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>2093212899</td>\n",
              "      <td>Simultaneous structure and texture image inpai...</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>[{'name': 'M. Bertalmio', 'org': 'Dept. de Tec...</td>\n",
              "      <td>596.0</td>\n",
              "      <td>Journal</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1488881187;1565233179;1569587969;1983661653;19...</td>\n",
              "      <td>https://doi.org/10.1109/TIP.2003.815261</td>\n",
              "      <td>IEEE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              id                                              title    year  \\\n",
              "9995  1964830323          An overview of JML tools and applications  2005.0   \n",
              "9996  2101699859  Countering code-injection attacks with instruc...  2003.0   \n",
              "9997  2124609748  Gaussian Process Dynamical Models for Human Mo...  2008.0   \n",
              "9998  2147343704  EVENODD: an efficient scheme for tolerating do...  1995.0   \n",
              "9999  2093212899  Simultaneous structure and texture image inpai...  2003.0   \n",
              "\n",
              "                                                authors  n_citation  \\\n",
              "9995  [{'name': 'Lilian Burdy', 'org': 'INRIA Sophia...       596.0   \n",
              "9996  [{'name': 'Gaurav S. Kc', 'org': 'Columbia Uni...       596.0   \n",
              "9997  [{'name': 'J.M. Wang', 'org': 'University of T...       596.0   \n",
              "9998  [{'name': 'M. Blaum', 'org': 'IBM Almaden Rese...       596.0   \n",
              "9999  [{'name': 'M. Bertalmio', 'org': 'Dept. de Tec...       596.0   \n",
              "\n",
              "        doc_type  reference_count  \\\n",
              "9995  Conference             82.0   \n",
              "9996  Conference             42.0   \n",
              "9997  Conference             61.0   \n",
              "9998     Journal              9.0   \n",
              "9999     Journal             22.0   \n",
              "\n",
              "                                             references  \\\n",
              "9995  1486696980;1489778371;1492315860;1498946538;14...   \n",
              "9996  186343359;1481758559;1499992849;1508969946;151...   \n",
              "9997  1505866674;1546296670;1598702468;1643263348;19...   \n",
              "9998  1530042190;1531975040;1820898047;1829547464;20...   \n",
              "9999  1488881187;1565233179;1569587969;1983661653;19...   \n",
              "\n",
              "                                            doi              publisher  \n",
              "9995  https://doi.org/10.1007/s10009-004-0167-4        Springer-Verlag  \n",
              "9996      https://doi.org/10.1145/948109.948146                    ACM  \n",
              "9997    https://doi.org/10.1109/TPAMI.2007.1167  IEEE Computer Society  \n",
              "9998          https://doi.org/10.1109/12.364531  IEEE Computer Society  \n",
              "9999    https://doi.org/10.1109/TIP.2003.815261                   IEEE  "
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sorted.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "2yc92UZO2Nr2"
      },
      "outputs": [],
      "source": [
        "def fill_nodes_citation(data, G):\n",
        "    for _, row in data.iterrows():\n",
        "        paper_id = row['id']\n",
        "        references_str = row['references']\n",
        "\n",
        "        # Convert the references string to a list of integers\n",
        "        references = [int(ref) for ref in str(references_str).split(';') if ref.isdigit()]\n",
        "\n",
        "        # Add edges to the graph if the reference exists in the dataset\n",
        "        for ref in references:\n",
        "            if ref in data['id'].values:\n",
        "                G.add_edge(paper_id, ref)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "QlWuFtypi8G3"
      },
      "outputs": [],
      "source": [
        "fill_nodes_citation(df_sorted, citation_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fill_nodes_collaboration(data, G):\n",
        "    for _, row in data.iterrows():\n",
        "        authors_str = row['authors']\n",
        "        title = row['id']  \n",
        "\n",
        "        try:\n",
        "            authors = ast.literal_eval(authors_str)\n",
        "            # Extract a unique identifier for each author (e.g., name or ID)\n",
        "            author_ids = [author.get('name') or author.get('id') for author in authors]\n",
        "        except ValueError:\n",
        "            author_ids = []\n",
        "\n",
        "        author_pairs = list(combinations(author_ids, 2))\n",
        "\n",
        "        for author_pair in author_pairs:\n",
        "            if G.has_edge(*author_pair):\n",
        "                G[author_pair[0]][author_pair[1]]['weight'] = G[author_pair[0]][author_pair[1]]['weight'] + 1 if 'weight' in G[author_pair[0]][author_pair[1]] else 1\n",
        "            else:\n",
        "                G.add_edge(author_pair[0], author_pair[1], weight=1, titles=title)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "8-VCU7Uxu6PO"
      },
      "outputs": [],
      "source": [
        "fill_nodes_collaboration(df_sorted, collaboration_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functionality 1 - Graph's features\n",
        "This function should examine a graph and report on some of its features. The input and report that this function should produce are shown below. \n",
        "\n",
        "Input: \n",
        "- The graph\n",
        "- The name of the graph\n",
        "\n",
        "Output: \n",
        "- The number of the nodes in the graph\n",
        "- The number of the edges in the graph\n",
        "- The graph density\n",
        "- The graph degree distribution\n",
        "- The average degree of the graph\n",
        "- The graph hubs (hubs are nodes having degrees more extensive than the 95th percentile of the degree distribution)\n",
        "- Whether the graph is dense or sparse\n",
        "\n",
        "-----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "def functionality_1(graph, graph_name):\n",
        "    if graph_name != \"citation_graph\" and graph_name != \"collaboration_graph\":\n",
        "        raise ValueError(\"Choose 'citation_graph' or 'collaboration_graph' as the second input value\")\n",
        "    \n",
        "    # Number of nodes\n",
        "    num_nodes = graph.number_of_nodes()\n",
        "\n",
        "    # Number of edges\n",
        "    num_edges = graph.number_of_edges()\n",
        "\n",
        "    # Graph density\n",
        "    density = nx.density(graph)\n",
        "\n",
        "    # Degree distribution\n",
        "    degree_sequence = [d for n, d in graph.degree()]\n",
        "    degree_distribution = dict(zip(*np.unique(degree_sequence, return_counts=True)))\n",
        "\n",
        "    # Average degree\n",
        "    avg_degree = np.mean(degree_sequence)\n",
        "\n",
        "    # 95th percentile of the degree distribution\n",
        "    percentile_95 = np.percentile(degree_sequence, 95)\n",
        "\n",
        "    # Graph hubs (nodes with degrees > 95th percentile)\n",
        "    hubs = [node for node, degree in graph.degree() if degree > percentile_95]\n",
        "\n",
        "    # Whether the graph is dense or sparse\n",
        "    graph_type = \"dense\" if density >= 0.5 else \"sparse\"\n",
        "    \n",
        "    return num_nodes, num_edges, density, degree_distribution, avg_degree, hubs, graph_type\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functionality 2 - Nodes' contribution \n",
        "Using this functionality, you will identify the papers/authors who have significantly contributed to this field of study. For this analysis, focusing solely on the number of citations for the paper or the number of collaborations of the authors can be misleading. You will examine this using various centrality measurements. \n",
        "\n",
        "Input:\n",
        "- The graph\n",
        "- A node of the graph (paper/author)\n",
        "- The name of the graph\n",
        "\n",
        "Output: \n",
        "- The centrality of the node, calculated based on the following centrality measurements:\n",
        "   - Betweeness\n",
        "   - PageRank\n",
        "   - ClosenessCentrality\n",
        "   - DegreeCentrality\n",
        "\n",
        "------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def functionality_2(graph, node, graph_name):\n",
        "    \n",
        "    if graph_name != \"citation_graph\" and graph_name != \"collaboration_graph\":\n",
        "        raise ValueError(\"Choose 'citation_graph' or 'collaboration_graph' as the second input value\")\n",
        "    \n",
        "    if node not in graph.nodes:\n",
        "        raise ValueError(f\"node '{node}' not found in the graph.\")\n",
        "    \n",
        "    \n",
        "    centrality_measures = {}\n",
        "\n",
        "    # Betweenness Centrality\n",
        "    betweenness_centrality = nx.betweenness_centrality(graph)\n",
        "    centrality_measures['Betweenness'] = betweenness_centrality.get(node, 0)\n",
        "\n",
        "    # PageRank\n",
        "    pagerank_centrality = nx.pagerank(graph)\n",
        "    centrality_measures['PageRank'] = pagerank_centrality.get(node, 0)\n",
        "\n",
        "    # Closeness Centrality\n",
        "    closeness_centrality = nx.closeness_centrality(graph)\n",
        "    centrality_measures['ClosenessCentrality'] = closeness_centrality.get(node, 0)\n",
        "\n",
        "    # Degree Centrality\n",
        "    degree_centrality = nx.degree_centrality(graph)\n",
        "    centrality_measures['DegreeCentrality'] = degree_centrality.get(node, 0)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\n",
        "    print(f\"Centrality Measures for {node} in {graph_name}:\")\n",
        "    for measure, value in centrality_measures.items():\n",
        "        print(f\"{measure}: {value}\")\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    return centrality_measures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functionality 3 - Shortest ordered walk  \n",
        "\n",
        "Input:  \n",
        "- The graph data \n",
        "- A sequence of authors\\_a = [a\\_2, ..., a\\_{n-1}]\n",
        "- Initial node a\\_1 and an end node a\\_n\n",
        "- $N$: denoting the top $N$ authors whose data should be considered\n",
        " \n",
        "Output: \n",
        "- The shortest walk of collaborations you need to read to get from author a\\_1 to author a\\_n and the papers you need to cross to realize this walk.\n",
        "\n",
        "---------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This helper function is useful in functionality 3 and 4 in order to get the top N authors based on papers published"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "for author in collaboration_graph.nodes:\n",
        "    collaboration_graph.nodes[author]['papers_published'] = len(list(collaboration_graph.neighbors(author)))\n",
        "def get_top_nodes(graph, N):\n",
        "        return sorted(graph.nodes, key=lambda x: graph.nodes[x]['papers_published'], reverse=True)[:N]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function uses a Breadth-First Search (BFS) approach to explore the graph and find the shortest path between two nodes.\n",
        "It then performs BFS to find the shortest path and associated titles from the starting node to the first intermediate node.\n",
        "Subsequently, it iterates through the intermediate nodes, calculating the shortest path and titles for each pair.\n",
        "Finally, it computes the shortest path and titles from the last intermediate node to the destination node, appending the results to the combined_path dictionary.\n",
        "If any segment of the path is not found, the function returns a corresponding error message.\n",
        "The final result is a dictionary containing the combined path and associated titles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "def functionality_3(graph, start, end, intermediate_nodes, N):\n",
        "    combined_path = {'path': [], 'titles': []}\n",
        "\n",
        "    # Helper function to perform BFS\n",
        "    def BFS(graph, start, end, top_nodes):\n",
        "        path_queue = deque()\n",
        "\n",
        "        # Each element in the queue is a tuple (current_path, titles_list)\n",
        "        temp_path = ([start], [])\n",
        "        path_queue.append(temp_path)\n",
        "\n",
        "        while path_queue:\n",
        "            # Dequeue the next path and its associated titles\n",
        "            current_path, current_titles = path_queue.popleft()\n",
        "            last_node = current_path[-1]\n",
        "\n",
        "            # Check if the last node in the path is the target\n",
        "            if last_node == end:\n",
        "                return current_path, current_titles\n",
        "\n",
        "            # Explore neighbors of the last node\n",
        "            for link_node, edge_data in graph[last_node].items():\n",
        "                # Check if the neighbor is not already in the current path and is in the top N nodes\n",
        "                if link_node not in current_path and link_node in top_nodes:\n",
        "                    # Extend the path and titles with the new neighbor and its associated titles\n",
        "                    new_path = current_path + [link_node]\n",
        "                    new_titles = current_titles + [edge_data.get('titles', [])]\n",
        "                    path_queue.append((new_path, new_titles))\n",
        "\n",
        "        # If no valid path is found\n",
        "        return [], []\n",
        "\n",
        "\n",
        "    # Get the top N nodes based on papers published\n",
        "    top_nodes = get_top_nodes(graph, N)\n",
        "\n",
        "    # Calculate the first path and titles\n",
        "    first_path, first_titles = BFS(graph, start, intermediate_nodes[0], top_nodes)\n",
        "    combined_path['path'] += first_path\n",
        "    combined_path['titles'] += first_titles\n",
        "\n",
        "    if not first_path:\n",
        "        return \"There is no such path.\"\n",
        "\n",
        "    start = intermediate_nodes[0]\n",
        "\n",
        "    # Iterate through intermediate nodes and calculate the shortest path for each pair\n",
        "    for intermediate_node in intermediate_nodes[1:]:\n",
        "        path, titles = BFS(graph, start, intermediate_node, top_nodes)\n",
        "        combined_path['path'] += path[1:]\n",
        "        combined_path['titles'] += titles\n",
        "\n",
        "        if not path:\n",
        "            return \"There is no such path.\"\n",
        "\n",
        "        start = intermediate_node\n",
        "\n",
        "    # Calculate the shortest path from the last intermediate node to the end\n",
        "    last_path, last_titles = BFS(graph, intermediate_nodes[-1], end, top_nodes)\n",
        "    combined_path['path'] += last_path[1:]\n",
        "    combined_path['titles'] += last_titles\n",
        "\n",
        "    if not last_path:\n",
        "        return \"There is no such path.\"\n",
        "\n",
        "    return combined_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functionality 4 - Disconnecting Graphs\n",
        "\n",
        "Input: \n",
        "- The graph data \n",
        "- authorA: a paper to which will relate sub-graph G\\_a\n",
        "- authorB: a paper to which will relate sub-graph G\\_b\n",
        "- $N$: denoting the top $N$ authors that their data should be considered\n",
        "\n",
        "Output:\n",
        "- The minimum number of edges (by considering their weights) required to disconnect the original graph in two disconnected subgraphs: G\\_a and G\\_b.\n",
        "\n",
        "---------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Ford-Fulkerson algorithm is used to find the maximum flow in a flow network. To disconnect graphs, one can modify this algorithm by identifying and removing edges with the maximum flow. This process is repeated until the remaining graph becomes disconnected\n",
        "\n",
        "Directed graphs provide a clear flow direction for the Ford-Fulkerson algorithm, essential for determining and augmenting flow paths. Undirected graphs lack this directional information, hindering the algorithm's ability to establish consistent flow directions and compute maximum flows effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function converts an undirected graph into a directed graph.\n",
        "It ensures consistent flow directions for the Ford-Fulkerson algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def convert_to_directed(original_graph):\n",
        "    # Create a new directed graph\n",
        "    directed_graph = nx.DiGraph()\n",
        "    \n",
        "    # Iterate over the edges of the original graph\n",
        "    for source, target, edge_data in original_graph.edges(data=True):\n",
        "        edge_weight = edge_data.get('weight')\n",
        "        # Create directed edges in both directions\n",
        "        directed_graph.add_edge(source, target, weight=edge_weight)\n",
        "        directed_graph.add_edge(target, source, weight=edge_weight)  \n",
        "    \n",
        "    return directed_graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This function performs a Breadth-First Search (BFS) to find augmenting paths.\n",
        "It updates the parents dictionary to keep track of the path from start to end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def BFS(graph, parents, start, end):\n",
        "    visited = set()\n",
        "    queue = deque([start])\n",
        "    visited.add(start)\n",
        "\n",
        "    while queue:\n",
        "        current_node = queue.popleft()\n",
        "\n",
        "        # Check if the current node is the end\n",
        "        if current_node == end:\n",
        "            break\n",
        "\n",
        "        # Explore neighbors for potential paths\n",
        "        for neighbor in graph.neighbors(current_node):\n",
        "            # Check if the edge has remaining capacity and has not been visited yet\n",
        "            if neighbor not in visited and graph[current_node][neighbor].get('weight') > 0:\n",
        "                queue.append(neighbor)\n",
        "                visited.add(neighbor)\n",
        "                parents[neighbor] = current_node\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function implements the modified version of the Ford-Fulkerson algorithm to find the maximum flow in a graph in order to find the minimum cut.\n",
        "It modifies the graph by iteratively identifying augmenting paths and updating residual capacities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def ford_fulkerson(graph, start, end):\n",
        "    # Create the residual graph, ensuring it is directed\n",
        "    residual_graph = convert_to_directed(graph.copy())\n",
        "    \n",
        "    # Initialization\n",
        "    parents = {}\n",
        "    max_flow = 0\n",
        "\n",
        "    # Continue until no more augmenting paths can be found\n",
        "    while True:\n",
        "        parents = {}\n",
        "        BFS(residual_graph, parents, start, end)\n",
        "        \n",
        "        if end not in parents:\n",
        "            break\n",
        "        \n",
        "        path_flow = float('inf')\n",
        "        current_node = end\n",
        "        \n",
        "        # Update the minimum residual capacity along the path and accumulate the maximum flow\n",
        "        for current_node in iter(lambda: parents[current_node], start):\n",
        "            path_flow = min(path_flow, residual_graph[parents[current_node]][current_node]['weight'])\n",
        "\n",
        "        max_flow += path_flow\n",
        "\n",
        "        # Update residual capacities along the path\n",
        "        current_node = end\n",
        "        while current_node != start:\n",
        "            parent_node = parents[current_node]\n",
        "            residual_graph[parent_node][current_node]['weight'] -= path_flow\n",
        "            residual_graph[current_node][parent_node]['weight'] += path_flow\n",
        "            current_node = parent_node\n",
        "        \n",
        "\n",
        "    # Identify edges with zero residual capacity in the forward direction, forming the minimum cut\n",
        "    cut_edges = set()\n",
        "    for u in residual_graph.nodes():\n",
        "        for v, edge_data in residual_graph[u].items():\n",
        "            if edge_data['weight'] == 0:\n",
        "                cut_edges.add((u, v))\n",
        "\n",
        "    return cut_edges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "def functionality_4(graph, authorA, authorB, N):\n",
        "    # Get the top N nodes based on papers published\n",
        "    top_nodes = get_top_nodes(graph, N)\n",
        "    subgraph = graph.subgraph(set(top_nodes))\n",
        "\n",
        "    # Check if authorA and authorB are in the graph\n",
        "    if authorA not in subgraph.nodes:\n",
        "        raise ValueError(f\"Author '{authorA}' not found in the graph.\")\n",
        "    elif authorB not in subgraph.nodes:\n",
        "        raise ValueError(f\"Author '{authorB}' not found in the graph.\")\n",
        "    \n",
        "    # Modified Ford-Fulkerson algorithm \n",
        "    modified_graph = subgraph.copy()\n",
        "    min_cut = ford_fulkerson(modified_graph, authorA, authorB)\n",
        "    \n",
        "    # Actually removing the edges\n",
        "    modified_graph.remove_edges_from(min_cut)\n",
        "    \n",
        "    return len(min_cut)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functionality 5 - Extracting Communities\n",
        "\n",
        "Input: \n",
        "- The graph data \n",
        "- $N$: denoting the top $N$ papers that their data should be considered\n",
        "- Paper\\_1: denoting the name of one of the papers \n",
        "- Paper\\_2: denoting the name of one of the papers\n",
        "\n",
        "Output:\n",
        "- The minimum number of edges that should be removed to form communities\n",
        "- A list of communities, each containing a list of papers that belong to them.\n",
        "- Whether the Paper\\_1 and Paper\\_2 belongs to the same community. \n",
        "\n",
        "------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this function systematically examines every edge in the graph, determining whether it connects nodes from distinct communities. By doing so, it calculates the count of edges that would need to be cut to achieve the desired partition of the graph into communities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_cut_edges(graph, communities):\n",
        "    cut_edges = 0\n",
        "    \n",
        "    for edge in graph.edges():\n",
        "        node1, node2 = edge\n",
        "        community_node1 = get_community(node1, communities)\n",
        "        community_node2 = get_community(node2, communities)\n",
        "        # check if they are in the same community\n",
        "        if community_node1 != community_node2:\n",
        "            cut_edges += 1\n",
        "    \n",
        "    return cut_edges\n",
        "\n",
        "def get_community(node, communities):\n",
        "    # Find the community to which the node belongs\n",
        "    for community, nodes in enumerate(communities):\n",
        "        if node in nodes:\n",
        "            return community\n",
        "    \n",
        "    return None  # Node not found in any community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This functionality uses Clauset-Newman-Moore greedy modularity maximization to find the community partition with the largest modularity.\n",
        "\n",
        "The modularity of a graph given a partition (the communities) is a parameter that quantifies the degree of internal connectivity within communities compared to connections between communities. the formula is:\n",
        "$$\n",
        "Mod = \\sum_{c=1}^{n} \\left( \\frac{L_c}{m} - \\gamma \\left( \\frac{k_c}{2m} \\right)^2 \\right)\n",
        "$$\n",
        "where the sum iterates over all communities $c$, $m$ is the number of edges, $L_c$ is the number of intra-community links for community $c$, $k_c$ is the sum of degrees of the nodes in community $c$, and $\\gamma$ is the resolution parameter (in this case 1, but it could be changed).\n",
        "\n",
        "Greedy modularity maximization begins with each node in its own community and repeatedly joins the pair of communities that lead to the largest modularity until no further increase in modularity is possible \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def greedy_modularity_communities(G):\n",
        "    # First create one community for each node\n",
        "    communities = [frozenset([u]) for u in G.nodes()]\n",
        "    # Track merges\n",
        "    merges = []\n",
        "    \n",
        "    old_modularity = None\n",
        "    new_modularity = nx.algorithms.community.modularity(G, communities)\n",
        "    \n",
        "    # Merge communities until no improvement is possible\n",
        "    while old_modularity is None or new_modularity > old_modularity:\n",
        "        # Save modularity for comparison\n",
        "        old_modularity = new_modularity\n",
        "        # Find best pair to merge\n",
        "        trial_communities = list(communities)\n",
        "        to_merge = None\n",
        "        for i, u in enumerate(communities):\n",
        "            for j, v in enumerate(communities):\n",
        "                # Skip i==j and empty communities\n",
        "                if j <= i or len(u) == 0 or len(v) == 0:\n",
        "                    continue\n",
        "                # Merge communities u and v\n",
        "                trial_communities[j] = u | v\n",
        "                trial_communities[i] = frozenset([])\n",
        "                trial_modularity = nx.algorithms.community.modularity(G, trial_communities)\n",
        "                if trial_modularity >= new_modularity:\n",
        "                    # Check if strictly better or tie\n",
        "                    if trial_modularity > new_modularity:\n",
        "                        # Found new best, save modularity and group indexes\n",
        "                        new_modularity = trial_modularity\n",
        "                        to_merge = (i, j, new_modularity - old_modularity)\n",
        "                    elif to_merge and min(i, j) < min(to_merge[0], to_merge[1]):\n",
        "                        # Break ties by choosing pair with lowest min id\n",
        "                        new_modularity = trial_modularity\n",
        "                        to_merge = (i, j, new_modularity - old_modularity)\n",
        "                # Un-merge\n",
        "                trial_communities[i] = u\n",
        "                trial_communities[j] = v\n",
        "        if to_merge is not None:\n",
        "            # If the best merge improves modularity, use it\n",
        "            merges.append(to_merge)\n",
        "            i, j, dq = to_merge\n",
        "            u, v = communities[i], communities[j]\n",
        "            communities[j] = u | v\n",
        "            communities[i] = frozenset([])\n",
        "    # Remove empty communities and sort\n",
        "    return sorted((c for c in communities if len(c) > 0), key=len, reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This selection was deliberate, distinguishing it from alternative methods such as Newman's algorithm, which ceases when the graph is partitioned into disconnected subgraphs. Utilizing Newman's algorithm on our graph, for instance, would terminate prematurely due to the existence of multiple disconnected subgraphs, resulting in a partition that merely replicates these subgraphs as communities. The Clauset-Newman-Moore approach, on the other hand, ensures a partition with favorable characteristics, avoiding premature halting and providing a more meaningful community structure even in the presence of disconnected subgraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "def functionality_5(graph, paper_1, paper_2, N):\n",
        "    \n",
        "    # Extract the top N authors according to degree and create a subgraph\n",
        "    top_papers = sorted(graph.degree, key=lambda x: x[1], reverse=True)[:N]\n",
        "    top_nodes = [author[0] for author in top_papers]\n",
        "    subgraph = graph.subgraph(top_nodes)\n",
        "    \n",
        "    # Check if paper_1 and paper_2 are nodes of the graph (Top N)\n",
        "    if paper_1 not in subgraph.nodes:\n",
        "        raise ValueError(f\"Node '{paper_1}' not found in the graph.\")\n",
        "\n",
        "    if paper_2 not in subgraph.nodes:\n",
        "        raise ValueError(f\"Node '{paper_2}' not found in the graph.\")\n",
        "    \n",
        "    # Using the Clauset-Newman-Moore greedy modularity maximization\n",
        "    c = list(greedy_modularity_communities(subgraph.copy()))\n",
        "    \n",
        "    # find the nodes forming the communities\n",
        "    communities = []\n",
        "    for i in c:\n",
        "        communities.append(list(i))\n",
        "\n",
        "    # Check if Paper_1 and Paper_2 belong to the same community\n",
        "    same_community = any(paper_1 in community and paper_2 in community for community in communities)\n",
        "    \n",
        "    # Using the above function to get the number of edges cut\n",
        "    number_edges_cut = count_cut_edges(subgraph, communities)\n",
        "    \n",
        "    return number_edges_cut, communities, same_community\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
